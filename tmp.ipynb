{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'SearchGenerator'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-3-85de1e07f60b>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mutils\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtabularModel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtitanicDataset\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 10\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mray\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtune\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     11\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mray\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtune\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mschedulers\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mASHAScheduler\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mray\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtune\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msuggest\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbayesopt\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mBayesOptSearch\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\FER\\lib\\site-packages\\ray\\tune\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mray\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtune\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0merror\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mTuneError\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mray\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtune\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtune\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mrun_experiments\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrun\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mray\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtune\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexperiment\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mExperiment\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mray\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtune\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0manalysis\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mExperimentAnalysis\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mAnalysis\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mray\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtune\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstopper\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mStopper\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mEarlyStopping\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\FER\\lib\\site-packages\\ray\\tune\\tune.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mray\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtune\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0manalysis\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mExperimentAnalysis\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mray\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtune\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msuggest\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mBasicVariantGenerator\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 7\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mray\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtune\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msuggest\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msuggestion\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mSearcher\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mSearchGenerator\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      8\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mray\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtune\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrial\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mTrial\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mray\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtune\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrainable\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mTrainable\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mImportError\u001B[0m: cannot import name 'SearchGenerator'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from utils import tabularModel, titanicDataset\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.suggest.bayesopt import BayesOptSearch\n",
    "\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(os.path.join('data', 'titanic', 'test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config):\n",
    "    batch_size = int(config['batch_size'])\n",
    "    num_embedding = int(config['num_embedding'])\n",
    "    learning_rate = config['learning_rate']\n",
    "\n",
    "    dataset = titanicDataset(os.path.join('data', 'titanic', 'train.csv'))\n",
    "    train_dataset, valid_dataset = torch.utils.data.random_split(dataset,\n",
    "                                                                 [int(0.8 * len(dataset)),\n",
    "                                                                  len(dataset) - int(0.8 * len(dataset))])\n",
    "    train_dataloader = DataLoader(train_dataset,\n",
    "                                  batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    valid_dataloader = DataLoader(valid_dataset,\n",
    "                                 batch_size=len(valid_dataset), shuffle=False, num_workers=4)\n",
    "    categorical_columns = ['Pclass', 'Sex', 'Embarked', 'binned_family']\n",
    "    categories = [3, 2, 3, 4]\n",
    "    continuous_columns = ['Fare', 'Age']\n",
    "    embedding_size = list(zip(categories, [num_embedding] * 4))\n",
    "    model = tabularModel(embedding_size, categorical_columns, continuous_columns).double()\n",
    "    optim = torch.optim.Adagrad(model.parameters(), lr=learning_rate, weight_decay=0.95)\n",
    "    lossfunc = nn.CrossEntropyLoss()\n",
    "    num_epoch = 300\n",
    "    best_accu = 0\n",
    "    cnt = 0\n",
    "    model.train()\n",
    "    for epoch in range(num_epoch):\n",
    "        for i, (x, y) in enumerate(train_dataloader):\n",
    "            optim.zero_grad()\n",
    "            pred = model(x)\n",
    "            loss = lossfunc(pred, y.squeeze().long())\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            if i % 10 == 0 or i == (len(train_dataset) / 16) - 1 :\n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "                    x, y = next(iter(valid_dataloader))\n",
    "                    pred = model(x)\n",
    "                    valid_loss = lossfunc(pred, y.squeeze().long())\n",
    "                    accu = (pred.argmax(-1) == y.squeeze()).sum().item() / len(valid_dataset)\n",
    "                print(\"Epoch: {} | Train loss: {:.4f} | Valid loss: {:.4f} | Valid Accu: {:.4f}\".format(\n",
    "                    epoch, loss.item(), valid_loss.item(), accu\n",
    "                ))\n",
    "                model.train()\n",
    "        print(\"\\n {:.4f} {:.4f} {} \\n\".format(1.02 * best_accu, accu, cnt))\n",
    "        if accu >= best_accu:\n",
    "            cnt = 0\n",
    "            best_accu = accu\n",
    "            # torch.save(model.state_dict(), os.path.join('model', 'torch_nn',\n",
    "            #                                             'model-{}.pt'.format(epoch)))\n",
    "        else:\n",
    "            cnt += 1\n",
    "\n",
    "        if cnt == 50:\n",
    "            print(\"Early stopped!\")\n",
    "            break\n",
    "    return best_accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def raytune_trainable(config, checkpoint_dir=None):\n",
    "    tune.report(accu=train(config))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    'batch_size': (2, 64),\n",
    "    'num_embedding': (4, 256),\n",
    "    'learning_rate': (1e-4, 0.1)\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "algo = BayesOptSearch(space=search_space, metric='accu', mode='max', random_search_steps=10)\n",
    "start = time.time()\n",
    "analysis = tune.run(raytune_trainable, search_alg=algo, num_samples=300, checkpoint_freq=4,\n",
    "                    checkpoint_at_end=True,\n",
    "                    local_dir=os.curdir,\n",
    "                    # resources_per_trial={'cpu': 9},\n",
    "                    scheduler = ASHAScheduler(\n",
    "                        metric='accu',\n",
    "                        mode='max')\n",
    "                   )\n",
    "end = time.time()\n",
    "print(end-start)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}